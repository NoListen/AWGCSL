{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a347e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from awgcsl.envs import *\n",
    "from awgcsl.envs.multi_world_wrapper import PointGoalWrapper, SawyerGoalWrapper, ReacherGoalWrapper\n",
    "import awgcsl.envs\n",
    "import awgcsl\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from gym.envs.registration import register\n",
    "# from argparse import ArgumentParser\n",
    "# def parse_args():\n",
    "#     parser = ArgumentParser(description='train args')\n",
    "#     parser.add_argument('-en','--env_name', type=str, default=None)\n",
    "#     parser.add_argument('-r','--repeat',type=int,default=None)\n",
    "#     return parser.parse_args()\n",
    "\n",
    "\n",
    "def register_envs():\n",
    "    register(\n",
    "        id='SawyerReachXYZEnv-v1',\n",
    "        entry_point='awgcsl.envs.sawyer_reach:SawyerReachXYZEnv',\n",
    "        tags={\n",
    "            'git-commit-hash': '2d95c75',\n",
    "            'author': 'murtaza'\n",
    "        },\n",
    "        kwargs={\n",
    "            'hide_goal_markers': True,\n",
    "            'norm_order': 2,\n",
    "        },\n",
    "    )\n",
    "    register(\n",
    "        id='Point2DLargeEnv-v1',\n",
    "        entry_point='awgcsl.envs.point2d:Point2DEnv',\n",
    "        tags={\n",
    "            'git-commit-hash': '4efe2be',\n",
    "            'author': 'Vitchyr'\n",
    "        },\n",
    "        kwargs={\n",
    "            'images_are_rgb': True,\n",
    "            'target_radius': 1,\n",
    "            'ball_radius': 0.5,\n",
    "            'boundary_dist':5,\n",
    "            'render_onscreen': False,\n",
    "            'show_goal': True,\n",
    "            'render_size':512,\n",
    "            'get_image_base_render_size': (48, 48),\n",
    "            'bg_color': 'white',\n",
    "        },\n",
    "    )\n",
    "    register(\n",
    "        id='Point2D-FourRoom-v1',\n",
    "        entry_point='awgcsl.envs.point2d:Point2DWallEnv',\n",
    "        kwargs={\n",
    "            'action_scale': 1,\n",
    "            'wall_shape': 'four-room-v1', \n",
    "            'wall_thickness': 0.30,\n",
    "            'target_radius':1,\n",
    "            'ball_radius':0.5,\n",
    "            'boundary_dist':5,\n",
    "            'render_size': 512,\n",
    "            'wall_color': 'darkgray',\n",
    "            'bg_color': 'white',\n",
    "            'images_are_rgb': True,\n",
    "            'render_onscreen': False,\n",
    "            'show_goal': True,\n",
    "            'get_image_base_render_size': (48, 48),\n",
    "        },\n",
    "    )\n",
    "    # register gcsl envs\n",
    "    register(\n",
    "        id='SawyerDoor-v0',\n",
    "        entry_point='awgcsl.envs.sawyer_door:SawyerDoorGoalEnv',\n",
    "    )\n",
    "register_envs()\n",
    "\"\"\"\n",
    "ref: Schulman, John, et al. \"Proximal policy optimization algorithms.\" arXiv preprint arXiv:1707.06347 (2017).\n",
    "ref: https://github.com/Jiankai-Sun/Proximal-Policy-Optimization-in-Pytorch/blob/master/ppo.py\n",
    "ref: https://github.com/openai/baselines/tree/master/baselines/ppo2\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join as joindir\n",
    "from os import makedirs as mkdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import math\n",
    "from IPython import embed\n",
    "\n",
    "for env_id in [args.env_name]:\n",
    "    env = gym.make(env_id)\n",
    "\n",
    "    if env_id.startswith('Fetch'):\n",
    "        env._max_episode_steps = 50\n",
    "    elif env_id.startswith('Sawyer'):\n",
    "        from awgcsl.envs.multi_world_wrapper import SawyerGoalWrapper\n",
    "        env = SawyerGoalWrapper(env)\n",
    "        if not hasattr(env, '_max_episode_steps'):\n",
    "            env = gym.wrappers.TimeLimit(env, max_episode_steps=50)\n",
    "    elif env_id.startswith('Point2D'):\n",
    "        from awgcsl.envs.multi_world_wrapper import PointGoalWrapper\n",
    "        env = gym.wrappers.TimeLimit(env, max_episode_steps=50)\n",
    "        env = PointGoalWrapper(env)\n",
    "    elif env_id.startswith('Reacher'):\n",
    "        from awgcsl.envs.multi_world_wrapper import ReacherGoalWrapper\n",
    "        env._max_episode_steps = 50\n",
    "        env = ReacherGoalWrapper(env)\n",
    "    else:\n",
    "        env = gym.wrappers.TimeLimit(env, max_episode_steps=50)\n",
    "\n",
    "    class RewardWrapper(gym.RewardWrapper):\n",
    "        def __init__(self, env):\n",
    "            super().__init__(env)\n",
    "\n",
    "        def reward(self, rew):\n",
    "            if rew != 0 and rew!= -1:\n",
    "                print(rew)\n",
    "            assert rew == 0 or rew == -1, 'input reward should be -1/0'\n",
    "            return rew + 1\n",
    "\n",
    "    env = RewardWrapper(env)\n",
    "    \n",
    "\n",
    "    LR_PPO = 5e-4\n",
    "    LR_HID = 0.0\n",
    "    FUTURE_P = 0.0\n",
    "\n",
    "    for repeat in range(args.repeat, args.repeat+1):\n",
    "        class args(object):\n",
    "            seed = 1234 + repeat\n",
    "            num_episode = 400 if ('SawyerDoor' in env_id or 'Reacher' in env_id) else 50\n",
    "            batch_size = 250\n",
    "            max_step_per_round = 50\n",
    "            gamma = 0.98\n",
    "            lamda = 0.97\n",
    "            log_num_episode = 1\n",
    "            num_epoch = 10\n",
    "            minibatch_size = 25\n",
    "            clip = 0.2\n",
    "            loss_coeff_value = 0.5\n",
    "            loss_coeff_entropy = 0.01\n",
    "\n",
    "            lr_ppo = LR_PPO\n",
    "            lr_hid = LR_HID\n",
    "            future_p = FUTURE_P # param of HER\n",
    "            \n",
    "        \n",
    "\n",
    "            num_parallel_run = 1\n",
    "            # tricks\n",
    "            schedule_adam = 'linear'\n",
    "            schedule_clip = 'linear'\n",
    "            layer_norm = True\n",
    "            state_norm = True\n",
    "            advantage_norm = True\n",
    "            lossvalue_norm = True\n",
    "            replay_buffer_size_IER = 50000 \n",
    "\n",
    "        rwds = []\n",
    "        Succ_recorder = []\n",
    "        global losses\n",
    "        Horizon_list = [1]\n",
    "        losses = [[]]\n",
    "\n",
    "        Transition = namedtuple('Transition', ('state', 'value', 'action', 'logproba', 'mask', 'next_state', 'reward'))\n",
    "        EPS = 1e-6\n",
    "        RESULT_DIR = 'Result_PPO'\n",
    "        mkdir(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "        class RunningStat(object):\n",
    "            def __init__(self, shape):\n",
    "                self._n = 0\n",
    "                self._M = np.zeros(shape)\n",
    "                self._S = np.zeros(shape)\n",
    "\n",
    "            def push(self, x):\n",
    "                x = np.asarray(x)\n",
    "                assert x.shape == self._M.shape\n",
    "                self._n += 1\n",
    "                if self._n == 1:\n",
    "                    self._M[...] = x\n",
    "                else:\n",
    "                    oldM = self._M.copy()\n",
    "                    self._M[...] = oldM + (x - oldM) / self._n\n",
    "                    self._S[...] = self._S + (x - oldM) * (x - self._M)\n",
    "\n",
    "            @property\n",
    "            def n(self):\n",
    "                return self._n\n",
    "\n",
    "            @property\n",
    "            def mean(self):\n",
    "                return self._M\n",
    "\n",
    "            @property\n",
    "            def var(self):\n",
    "                return self._S / (self._n - 1) if self._n > 1 else np.square(self._M)\n",
    "\n",
    "            @property\n",
    "            def std(self):\n",
    "                return np.sqrt(self.var)\n",
    "\n",
    "            @property\n",
    "            def shape(self):\n",
    "                return self._M.shape\n",
    "\n",
    "\n",
    "        class ZFilter:\n",
    "            \"\"\"\n",
    "            y = (x-mean)/std\n",
    "            using running estimates of mean,std\n",
    "            \"\"\"\n",
    "\n",
    "            def __init__(self, shape, demean=True, destd=True, clip=10.0):\n",
    "                self.demean = demean\n",
    "                self.destd = destd\n",
    "                self.clip = clip\n",
    "\n",
    "                self.rs = RunningStat(shape)\n",
    "\n",
    "            def __call__(self, x, update=True):\n",
    "                if update: self.rs.push(x)\n",
    "                if self.demean:\n",
    "                    x = x - self.rs.mean\n",
    "                if self.destd:\n",
    "                    x = x / (self.rs.std + 1e-8)\n",
    "                if self.clip:\n",
    "                    x = np.clip(x, -self.clip, self.clip)\n",
    "                return x\n",
    "\n",
    "            def output_shape(self, input_space):\n",
    "                return input_space.shape\n",
    "\n",
    "\n",
    "        class ActorCritic(nn.Module):\n",
    "            def __init__(self, num_inputs, num_outputs, layer_norm=True):\n",
    "                super(ActorCritic, self).__init__()\n",
    "\n",
    "                self.actor_fc1 = nn.Linear(num_inputs, 256)\n",
    "                self.actor_fc2 = nn.Linear(256, 256)\n",
    "                self.actor_fc3 = nn.Linear(256, num_outputs)\n",
    "                self.actor_logstd = nn.Parameter(torch.zeros(1, num_outputs))\n",
    "\n",
    "                self.critic_fc1 = nn.Linear(num_inputs, 256)\n",
    "                self.critic_fc2 = nn.Linear(256, 256)\n",
    "                self.critic_fc3 = nn.Linear(256, 1)\n",
    "\n",
    "                if layer_norm:\n",
    "                    self.layer_norm(self.actor_fc1, std=1.0)\n",
    "                    self.layer_norm(self.actor_fc2, std=1.0)\n",
    "                    self.layer_norm(self.actor_fc3, std=0.01)\n",
    "\n",
    "                    self.layer_norm(self.critic_fc1, std=1.0)\n",
    "                    self.layer_norm(self.critic_fc2, std=1.0)\n",
    "                    self.layer_norm(self.critic_fc3, std=1.0)\n",
    "\n",
    "            @staticmethod\n",
    "            def layer_norm(layer, std=1.0, bias_const=0.0):\n",
    "                torch.nn.init.orthogonal_(layer.weight, std)\n",
    "                torch.nn.init.constant_(layer.bias, bias_const)\n",
    "\n",
    "            def forward(self, states):\n",
    "                action_mean, action_logstd = self._forward_actor(states)\n",
    "                critic_value = self._forward_critic(states)\n",
    "                return action_mean, action_logstd, critic_value\n",
    "\n",
    "            def _forward_actor(self, states):\n",
    "                x = torch.tanh(self.actor_fc1(states))\n",
    "                x = torch.tanh(self.actor_fc2(x))\n",
    "                action_mean = self.actor_fc3(x)\n",
    "                action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "                return action_mean, action_logstd\n",
    "\n",
    "            def _forward_critic(self, states):\n",
    "                x = torch.tanh(self.critic_fc1(states))\n",
    "                x = torch.tanh(self.critic_fc2(x))\n",
    "                critic_value = self.critic_fc3(x)\n",
    "                return critic_value\n",
    "\n",
    "            def select_action(self, action_mean, action_logstd, return_logproba=True):\n",
    "                action_std = torch.exp(action_logstd)\n",
    "                action = torch.normal(action_mean, action_std)\n",
    "                if return_logproba:\n",
    "                    logproba = self._normal_logproba(action, action_mean, action_logstd, action_std)\n",
    "                return action, logproba\n",
    "\n",
    "            @staticmethod\n",
    "            def _normal_logproba(x, mean, logstd, std=None):\n",
    "                if std is None:\n",
    "                    std = torch.exp(logstd)\n",
    "\n",
    "                std_sq = std.pow(2)\n",
    "                logproba = - 0.5 * math.log(2 * math.pi) - logstd - (x - mean).pow(2) / (2 * std_sq)\n",
    "                return logproba.sum(1)\n",
    "\n",
    "            def get_logproba(self, states, actions):\n",
    "                action_mean, action_logstd = self._forward_actor(states)\n",
    "                logproba = self._normal_logproba(actions, action_mean, action_logstd)\n",
    "                return logproba\n",
    "\n",
    "\n",
    "        class Memory(object):\n",
    "            def __init__(self):\n",
    "                self.memory = []\n",
    "\n",
    "            def push(self, *args):\n",
    "                self.memory.append(Transition(*args))\n",
    "\n",
    "            def sample(self):\n",
    "                return Transition(*zip(*self.memory))\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.memory)\n",
    "        class ReplayBuffer_imitation(object):\n",
    "            def __init__(self, capacity):\n",
    "                self.buffer = {'1step':deque(maxlen=capacity)}\n",
    "                self.capacity = capacity\n",
    "            def push(self, state, action, step_num):\n",
    "                try:\n",
    "                    self.buffer[step_num]\n",
    "                except:\n",
    "                    self.buffer[step_num] = deque(maxlen=self.capacity)\n",
    "                self.buffer[step_num].append((state, action))\n",
    "\n",
    "\n",
    "            def sample(self, batch_size,step_num):\n",
    "                state, action= zip(*random.sample(self.buffer[step_num], batch_size))\n",
    "                return np.stack(state), action\n",
    "\n",
    "            def lenth(self,step_num):\n",
    "                try:\n",
    "                    self.buffer[step_num]\n",
    "                except:\n",
    "                    return 0\n",
    "                return len(self.buffer[step_num])\n",
    "\n",
    "            def __len__(self,step_num):\n",
    "                try:\n",
    "                    self.buffer[step_num]\n",
    "                except:\n",
    "                    return 0\n",
    "                return len(self.buffer[step_num])\n",
    "\n",
    "\n",
    "        num_inputs = env.observation_space.spaces['observation'].shape[0] + env.observation_space.spaces['desired_goal'].shape[0] + env.observation_space.spaces['achieved_goal'].shape[0]# extended state\n",
    "        num_actions = env.action_space.shape[0]\n",
    "        network = ActorCritic(num_inputs, num_actions, layer_norm=args.layer_norm)\n",
    "        model_imitation = network\n",
    "\n",
    "        Horizon_list = [1]\n",
    "\n",
    "        def ppo(args):\n",
    "            num_inputs = env.observation_space.spaces['observation'].shape[0] + env.observation_space.spaces['desired_goal'].shape[0] + env.observation_space.spaces['achieved_goal'].shape[0]# extended state\n",
    "            num_actions = env.action_space.shape[0]\n",
    "\n",
    "            env.seed(args.seed)\n",
    "            torch.manual_seed(args.seed)\n",
    "\n",
    "            optimizer = opt.Adam(network.parameters(), lr=args.lr_ppo)\n",
    "            optimizer_imitation = opt.Adam(model_imitation.parameters(),lr = args.lr_hid )\n",
    "\n",
    "\n",
    "            running_state = ZFilter((num_inputs,), clip=5.0)\n",
    "\n",
    "\n",
    "            reward_record = []\n",
    "            global_steps = 0\n",
    "\n",
    "            lr_now = args.lr_ppo\n",
    "            clip_now = args.clip\n",
    "            ier_buffer = ReplayBuffer_imitation(args.replay_buffer_size_IER)\n",
    "            for i_episode in range(args.num_episode):\n",
    "                memory = Memory()\n",
    "                num_steps = 0\n",
    "                reward_list = []\n",
    "                len_list = []\n",
    "                Succ_num = 0\n",
    "\n",
    "                game_num = 0\n",
    "                succ_game = 0\n",
    "                while num_steps < args.batch_size:\n",
    "                    state = env.reset()\n",
    "                    game_num +=1\n",
    "                    state = np.concatenate((state['observation'],state['desired_goal'],state['achieved_goal'])) # state_extended\n",
    "\n",
    "                    if args.state_norm:\n",
    "                        state = running_state(state)\n",
    "                    reward_sum = 0\n",
    "                    episode = []\n",
    "                    env_list = []\n",
    "                    Succ_in_env = 0\n",
    "                    for t in range(args.max_step_per_round):\n",
    "                        action_mean, action_logstd, value = network(Tensor(state).unsqueeze(0))\n",
    "                        action, logproba = network.select_action(action_mean, action_logstd)\n",
    "\n",
    "                        action = action.data.numpy()[0]\n",
    "                        logproba = logproba.data.numpy()[0]\n",
    "                        #print('action:',action)\n",
    "                        if np.sum(np.isnan(action))>0:\n",
    "                            embed()\n",
    "                        next_state, reward, done, _ = env.step(action)\n",
    "                        if _['is_success'] !=0:\n",
    "                            Succ_in_env = 1\n",
    "                            Succ_num+=1\n",
    "                        next_state = np.concatenate((next_state['observation'],next_state['desired_goal'],next_state['achieved_goal']))\n",
    "\n",
    "                        reward_sum += reward\n",
    "                        if args.state_norm:\n",
    "                            next_state = running_state(next_state)\n",
    "                        mask = 0 if done else 1\n",
    "                        episode.append((state, value, action, logproba, mask, next_state, reward))\n",
    "                        memory.push(state, value, action, logproba, mask, next_state, reward)\n",
    "                        if done:\n",
    "                            break\n",
    "\n",
    "                        state = next_state\n",
    "                    succ_game += Succ_in_env\n",
    "\n",
    "                    #for ind,(state, value, action, logproba, mask, next_state, reward) in enumerate(episode):\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "                    num_steps += (t + 1)\n",
    "                    global_steps += (t + 1)\n",
    "                    reward_list.append(reward_sum)\n",
    "                    len_list.append(t + 1)\n",
    "                    Winrate = 1.0*succ_game/game_num\n",
    "                    Succ_recorder.append(Winrate)\n",
    "\n",
    "                reward_record.append({\n",
    "                    'episode': i_episode, \n",
    "                    'steps': global_steps, \n",
    "                    'meanepreward': np.mean(reward_list), \n",
    "                    'meaneplen': np.mean(len_list)})\n",
    "\n",
    "                rwds.extend(reward_list)\n",
    "                batch = memory.sample()\n",
    "                batch_size = len(memory)\n",
    "\n",
    "                SR = 1.0*Succ_num/num_steps\n",
    "\n",
    "                # step2: extract variables from trajectories\n",
    "                rewards = Tensor(batch.reward)\n",
    "                values = Tensor(batch.value)\n",
    "                masks = Tensor(batch.mask)\n",
    "                actions = Tensor(batch.action)\n",
    "                states = Tensor(batch.state)\n",
    "                oldlogproba = Tensor(batch.logproba)\n",
    "\n",
    "                returns = Tensor(batch_size)\n",
    "                deltas = Tensor(batch_size)\n",
    "                advantages = Tensor(batch_size)\n",
    "\n",
    "                prev_return = 0\n",
    "                prev_value = 0\n",
    "                prev_advantage = 0\n",
    "                for i in reversed(range(batch_size)):\n",
    "                    returns[i] = rewards[i] + args.gamma * prev_return * masks[i]\n",
    "                    deltas[i] = rewards[i] + args.gamma * prev_value * masks[i] - values[i]\n",
    "                    # ref: https://arxiv.org/pdf/1506.02438.pdf (generalization advantage estimate)\n",
    "                    advantages[i] = deltas[i] + args.gamma * args.lamda * prev_advantage * masks[i]\n",
    "\n",
    "                    prev_return = returns[i]\n",
    "                    prev_value = values[i]\n",
    "                    prev_advantage = advantages[i]\n",
    "                if args.advantage_norm:\n",
    "                    advantages = (advantages - advantages.mean()) / (advantages.std() + EPS)\n",
    "\n",
    "                for i_epoch in range(int(args.num_epoch * batch_size / args.minibatch_size)):\n",
    "                    # sample from current batch\n",
    "                    minibatch_ind = np.random.choice(batch_size, args.minibatch_size, replace=False)\n",
    "                    minibatch_states = states[minibatch_ind]\n",
    "                    minibatch_actions = actions[minibatch_ind]\n",
    "                    minibatch_oldlogproba = oldlogproba[minibatch_ind]\n",
    "                    minibatch_newlogproba = network.get_logproba(minibatch_states, minibatch_actions)\n",
    "                    minibatch_advantages = advantages[minibatch_ind]\n",
    "                    minibatch_returns = returns[minibatch_ind]\n",
    "                    minibatch_newvalues = network._forward_critic(minibatch_states).flatten()\n",
    "\n",
    "                    ratio =  torch.exp(minibatch_newlogproba - minibatch_oldlogproba)\n",
    "                    surr1 = ratio * minibatch_advantages\n",
    "                    surr2 = ratio.clamp(1 - clip_now, 1 + clip_now) * minibatch_advantages\n",
    "                    loss_surr = - torch.mean(torch.min(surr1, surr2))\n",
    "\n",
    "                    if args.lossvalue_norm:\n",
    "                        minibatch_return_6std = 6 * minibatch_returns.std() + EPS\n",
    "                        loss_value = torch.mean((minibatch_newvalues - minibatch_returns).pow(2)) / minibatch_return_6std\n",
    "                    else:\n",
    "                        loss_value = torch.mean((minibatch_newvalues - minibatch_returns).pow(2))\n",
    "\n",
    "                    loss_entropy = torch.mean(torch.exp(minibatch_newlogproba) * minibatch_newlogproba)\n",
    "\n",
    "                    total_loss = loss_surr + args.loss_coeff_value * loss_value + args.loss_coeff_entropy * loss_entropy\n",
    "                    optimizer.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "                if args.schedule_clip == 'linear':\n",
    "                    ep_ratio = 1 - (i_episode / args.num_episode)\n",
    "                    clip_now = args.clip * ep_ratio\n",
    "\n",
    "                if args.schedule_adam == 'linear':\n",
    "                    ep_ratio = 1 - (i_episode / args.num_episode)\n",
    "                    lr_now = args.lr_ppo * ep_ratio\n",
    "                    for g in optimizer.param_groups:\n",
    "                        g['lr'] = lr_now\n",
    "\n",
    "                if i_episode % args.log_num_episode == 0:\n",
    "                    print('Finished episode: {} Reward: {:.4f} Stay Rate{:.4f} SuccessRate{:.4f}' \\\n",
    "                        .format(i_episode, reward_record[-1]['meanepreward'],SR,Winrate))\n",
    "                    print('-----------------')\n",
    "\n",
    "            return reward_record\n",
    "\n",
    "        def test(args):\n",
    "            record_dfs = []\n",
    "            for i in range(args.num_parallel_run):\n",
    "                args.seed += 1\n",
    "                reward_record = pd.DataFrame(ppo(args))\n",
    "                reward_record['#parallel_run'] = i\n",
    "                record_dfs.append(reward_record)\n",
    "            record_dfs = pd.concat(record_dfs, axis=0)\n",
    "            record_dfs.to_csv(joindir(RESULT_DIR, 'ppo-record-env{}_repeat{}.csv'.format(env_id, repeat)))\n",
    "\n",
    "\n",
    "        test(args)\n",
    "        rwds_HER_HID= deepcopy(rwds)\n",
    "        Succ_recorder_HER_HID= deepcopy(Succ_recorder)\n",
    "        np.save('Result_PPO/method_{}_env{}_repeat{}'.format(METHOD,env_id,repeat),(rwds_HER_HID,Succ_recorder_HER_HID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4eac9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.79",
   "language": "python",
   "name": "py2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
